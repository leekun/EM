{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 来源\n",
    "\n",
    "** RL **:\n",
    "    \n",
    "    介绍：http://karpathy.github.io/2016/05/31/rl/\n",
    "    \n",
    "    代码：https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "    代码keras: https://github.com/keon/policy-gradient/blob/master/pg.py\n",
    "\n",
    "    中文：https://www.jiqizhixin.com/articles/2016-06-15\n",
    "\n",
    "** gym教程：**\n",
    "    \n",
    "    https://www.cnblogs.com/mandalalala/p/6227201.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gym 初步了解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-25T01:55:28.878353Z",
     "start_time": "2018-06-25T01:55:27.695733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "Discrete(6)\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# plt.ion()\n",
    "%matplotlib inline\n",
    "import gym\n",
    "env = gym.make('Pong-v0')\n",
    "init_observ = env.reset() #重置环境的状态，返回observations。\n",
    "print(init_observ.shape)\n",
    "print(env.action_space) # 查看这个环境中可用的 action 有多少个\n",
    "print(env.observation_space.shape)    # 查看这个环境中可用的 state 的 observation 有多少个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T07:26:12.180941Z",
     "start_time": "2018-06-22T07:26:12.175649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T08:38:34.906255Z",
     "start_time": "2018-06-21T08:38:34.891473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Discrete(2)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# 每一个游戏不同, 每个observation 不同\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space) # 查看这个环境中可用的 action 有多少个\n",
    "print(env.observation_space.shape)    # 查看这个环境中可用的 state 的 observation 有多少个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T07:29:37.737234Z",
     "start_time": "2018-06-22T07:29:37.344605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.03371105 -0.04982973  0.00835811  0.0419115 ]\n",
      "[ 0.03271446  0.14517138  0.00919634 -0.24812269]\n",
      "[ 0.03561789  0.34016079  0.00423388 -0.53789076]\n",
      "[ 0.0424211   0.53522296 -0.00652393 -0.82923664]\n",
      "[ 0.05312556  0.73043349 -0.02310866 -1.12396422]\n",
      "[ 0.06773423  0.92585062 -0.04558795 -1.423805  ]\n",
      "[ 0.08625124  1.1215056  -0.07406405 -1.73038077]\n",
      "[ 0.10868136  1.31739122 -0.10867166 -2.0451586 ]\n",
      "[ 0.13502918  1.51344811 -0.14957484 -2.36939576]\n",
      "[ 0.16529814  1.70954852 -0.19696275 -2.70407325]\n",
      "Episode finished after 10 timesteps\n",
      "[-0.04881488  0.04344371  0.04580856  0.00554066]\n",
      "[-0.04794601  0.23787979  0.04591938 -0.2723445 ]\n",
      "[-0.04318841  0.43231746  0.04047249 -0.55019801]\n",
      "[-0.03454206  0.62684825  0.02946853 -0.82985959]\n",
      "[-0.0220051   0.82155525  0.01287134 -1.1131308 ]\n",
      "[-0.00557399  1.01650583 -0.00939128 -1.40174837]\n",
      "[ 0.01475613  1.21174319 -0.03742625 -1.69735249]\n",
      "[ 0.03899099  1.40727619 -0.0713733  -2.00144734]\n",
      "[ 0.06713651  1.60306671 -0.11140224 -2.31535135]\n",
      "[ 0.09919785  1.79901451 -0.15770927 -2.64013563]\n",
      "Episode finished after 10 timesteps\n",
      "[-3.75483928e-03 -1.48567362e-02  4.34957751e-05 -4.20391829e-02]\n",
      "[-0.00405197  0.18026459 -0.00079729 -0.33470839]\n",
      "[-4.46682180e-04  3.75397881e-01 -7.49145561e-03 -6.27642630e-01]\n",
      "[ 0.00706128  0.57062359 -0.02004431 -0.92267544]\n",
      "[ 0.01847375  0.76601053 -0.03849782 -1.22158962]\n",
      "[ 0.03379396  0.96160682 -0.06292961 -1.52608222]\n",
      "[ 0.05302609  1.15742934 -0.09345125 -1.83772367]\n",
      "[ 0.07617468  1.35345137 -0.13020573 -2.15790843]\n",
      "[ 0.10324371  1.54958784 -0.1733639  -2.48779516]\n",
      "Episode finished after 9 timesteps\n",
      "[0.04753405 0.01969934 0.0447884  0.00845514]\n",
      "[ 0.04792803  0.2141513   0.0449575  -0.26976691]\n",
      "[ 0.05221106  0.40860381  0.03956216 -0.54793791]\n",
      "[ 0.06038314  0.60314829  0.02860341 -0.82789805]\n",
      "[ 0.0724461   0.79786773  0.01204544 -1.11144946]\n",
      "[ 0.08840346  0.99282939 -0.01018354 -1.40032947]\n",
      "[ 0.10826004  1.18807642 -0.03819013 -1.69617873]\n",
      "[ 0.13202157  1.38361751 -0.07211371 -2.00050168]\n",
      "[ 0.15969392  1.57941441 -0.11212374 -2.31461674]\n",
      "[ 0.19128221  1.77536661 -0.15841608 -2.63959475]\n",
      "Episode finished after 10 timesteps\n",
      "[-0.01990719  0.00957105  0.04747453  0.02557334]\n",
      "[-0.01971577  0.20398116  0.047986   -0.251761  ]\n",
      "[-0.01563615  0.39838622  0.04295078 -0.52893044]\n",
      "[-0.00766842  0.59287843  0.03237217 -0.80777606]\n",
      "[ 0.00418914  0.78754213  0.01621665 -1.09010286]\n",
      "[ 0.01993999  0.9824466  -0.00558541 -1.37765364]\n",
      "[ 0.03958892  1.17763786 -0.03313848 -1.67207807]\n",
      "[ 0.06314168  1.37312864 -0.06658005 -1.97489418]\n",
      "[ 0.09060425  1.56888607 -0.10607793 -2.28743981]\n",
      "[ 0.12198197  1.76481675 -0.15182672 -2.61081229]\n",
      "[ 0.15727831  1.96074908 -0.20404297 -2.94579567]\n",
      "Episode finished after 11 timesteps\n",
      "[ 0.0148591   0.04545325  0.04905613 -0.0066404 ]\n",
      "[ 0.01576817  0.2398386   0.04892333 -0.28345093]\n",
      "[ 0.02056494  0.43422985  0.04325431 -0.5603112 ]\n",
      "[ 0.02924953  0.62871888  0.03204808 -0.83905908]\n",
      "[ 0.04182391  0.82338892  0.0152669  -1.12149366]\n",
      "[ 0.05829169  1.01830736 -0.00716297 -1.40934896]\n",
      "[ 0.07865784  1.21351742 -0.03534995 -1.70426244]\n",
      "[ 0.10292819  1.4090279  -0.0694352  -2.00773566]\n",
      "[ 0.13110874  1.60480066 -0.10958991 -2.32108473]\n",
      "[ 0.16320476  1.80073538 -0.15601161 -2.64537869]\n",
      "[ 0.19921947  1.99665172 -0.20891918 -2.98136561]\n",
      "Episode finished after 11 timesteps\n",
      "[ 0.02645384  0.00616378  0.04007325 -0.00909134]\n",
      "[ 0.02657711  0.20068879  0.03989142 -0.28886623]\n",
      "[ 0.03059089  0.39521986  0.0341141  -0.56870571]\n",
      "[ 0.03849529  0.58984714  0.02273999 -0.85044917]\n",
      "[ 0.05029223  0.78465175  0.005731   -1.13589557]\n",
      "[ 0.06598526  0.97969826 -0.01698691 -1.42677562]\n",
      "[ 0.08557923  1.17502592 -0.04552242 -1.72471875]\n",
      "[ 0.10907975  1.37063812 -0.0800168  -2.03121205]\n",
      "[ 0.13649251  1.56648944 -0.12064104 -2.34754889]\n",
      "[ 0.1678223   1.76247006 -0.16759202 -2.67476565]\n",
      "Episode finished after 10 timesteps\n",
      "[-0.02215432  0.03646352  0.00537148  0.02394343]\n",
      "[-0.02142505  0.23150802  0.00585035 -0.26703991]\n",
      "[-1.67948918e-02  4.26545994e-01  5.09550841e-04 -5.57871860e-01]\n",
      "[-0.00826397  0.62166079 -0.01064789 -0.85039421]\n",
      "[ 0.00416924  0.8169263  -0.02765577 -1.14640629]\n",
      "[ 0.02050777  1.01239828 -0.0505839  -1.4476319 ]\n",
      "[ 0.04075574  1.20810446 -0.07953653 -1.755681  ]\n",
      "[ 0.06491782  1.40403284 -0.11465015 -2.07200357]\n",
      "[ 0.09299848  1.60011774 -0.15609023 -2.39783329]\n",
      "[ 0.12500084  1.79622328 -0.20404689 -2.73411981]\n",
      "Episode finished after 10 timesteps\n",
      "[-0.01643898 -0.03637393 -0.02228365 -0.02441369]\n",
      "[-0.01716646  0.15906039 -0.02277192 -0.32404323]\n",
      "[-0.01398525  0.35449906 -0.02925279 -0.6238196 ]\n",
      "[-0.00689527  0.55001696 -0.04172918 -0.92557006]\n",
      "[ 0.00410507  0.74567689 -0.06024058 -1.23106929]\n",
      "[ 0.01901861  0.94151974 -0.08486197 -1.54200071]\n",
      "[ 0.037849    1.13755322 -0.11570198 -1.85991225]\n",
      "[ 0.06060007  1.33373881 -0.15290023 -2.18616363]\n",
      "[ 0.08727484  1.52997632 -0.1966235  -2.5218635 ]\n",
      "Episode finished after 9 timesteps\n",
      "[-0.0234499  -0.03540839  0.02353706  0.03814293]\n",
      "[-0.02415807  0.15936827  0.02429992 -0.24702188]\n",
      "[-0.02097071  0.35413489  0.01935949 -0.53194209]\n",
      "[-0.01388801  0.54897927  0.00872064 -0.81846256]\n",
      "[-0.00290842  0.74398077 -0.00764861 -1.10838986]\n",
      "[ 0.01197119  0.93920241 -0.02981641 -1.40346242]\n",
      "[ 0.03075524  1.13468172 -0.05788565 -1.70531577]\n",
      "[ 0.05344888  1.3304199  -0.09199197 -2.01544017]\n",
      "[ 0.08005727  1.52636869 -0.13230077 -2.33512807]\n",
      "[ 0.11058465  1.72241458 -0.17900333 -2.66541009]\n",
      "Episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(10): # 玩10次\n",
    "    observation = env.reset()\n",
    "    for t in range(100):    # 每次至少100个时间步长，100个时间步长也有可能游戏没有结束哦。。。\n",
    "        env.render()        # 刷新当前环境，并显示\n",
    "        print(observation)  # 打印每次的观察值\n",
    "        action = 1#env.action_space.sample()  # 随机选取一个动作\n",
    "        observation, reward, done, info = env.step(action) # step 根据下一步的action，返回： 观察，奖励，是否结束一个环节，调试信息。\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T02:20:18.853569Z",
     "start_time": "2018-06-22T02:20:18.787016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization 权重使用xavier初始化\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T10:06:47.276561Z",
     "start_time": "2018-06-22T10:06:47.269418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_ for _ in reversed([1,2,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T08:52:16.282908Z",
     "start_time": "2018-06-21T08:52:16.159758Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" 处理图片,裁剪,下采样,改变大小,变换为0和1\"\"\"\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop \n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "    # 对于每一步所采样的动作，我们考虑未来所有回报的加权和，而且未来回报的重要性随时间以指数递减\n",
    "    \"\"\" take 1D float array of rewards and compxxxute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r) \n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):  \n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r \n",
    "\n",
    "def policy_forward(x):\n",
    "    \"\"\" 前向传播, 两层的策略网络\"\"\"\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-21T09:00:26.256165Z",
     "start_time": "2018-06-21T09:00:26.240669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.772770632091281"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T08:02:12.347865Z",
     "start_time": "2018-06-22T08:02:12.340248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<range_iterator at 0x10e02aed0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[1,2],[3,4]]\n",
    "np.vstack(a)\n",
    "reversed(range(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T10:23:13.645548Z",
     "start_time": "2018-06-22T10:23:13.637739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 18])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "b * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T10:22:38.026637Z",
     "start_time": "2018-06-22T10:22:34.194584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: 1.000000 !!!!!!!!\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "ep 0: game finished, reward: -1.000000\n",
      "(1623, 6400) (6400,)\n",
      "(1623, 200)\n",
      "(1623, 1) 0.0\n",
      "(1623, 1)\n",
      "(1623, 1) (1623, 1)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image. \n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D) #两帧画面的差\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability \n",
    "  aprob, h = policy_forward(x) #输入到策略网络并得到下一步动作的概率（动作action=2的概率） \n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice! 按照伯努利分布(aprob,1-aprob),选取一个动作\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"  # 把所有action==2 的y值为1, 定义这步动作的”实际“标签为1，为什么？\n",
    "  dlogps.append(y - aprob) #grad that encourages the action that was taken to be taken \n",
    "    #(see http://cs231n.github.io/neural-networks-2/#losses if confused)  就是sigmoid 对应 logloss的梯度\n",
    "    \n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action) # 得到其反馈\n",
    "  reward_sum += reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    print(epx.shape,xs[0].shape)\n",
    "    eph = np.vstack(hs)\n",
    "    print(eph.shape)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    print(epr.shape,drs[0])\n",
    "    #print(drs[:30])\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr) # 计算其所有的回报\n",
    "    print(discounted_epr.shape)\n",
    "    #break\n",
    "\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "    print(epdlogp.shape, discounted_epr.shape)\n",
    "    #break\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch # 梯度累加\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.items():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "  if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "    print(('ep %d: game finished, reward: %f'%(episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
